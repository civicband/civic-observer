# Unified Deploy Strategy for civic.band Infrastructure

**Date:** 2024-11-27
**Status:** Draft

## Overview

This document describes a unified blue-green deployment strategy for the civic.band infrastructure, covering civic-observer, corkboard (django + sites_datasette), and the shared Caddy reverse proxy.

## Current State

### Architecture

```
VPS
├── Caddy (systemd, standalone)
│   └── Single Caddyfile generated by civic-band CI (via clerk)
├── civic-observer/ (docker-compose)
│   ├── web (Django) - port 8888
│   ├── worker (RQ)
│   ├── postgres
│   └── redis
└── corkboard/ (docker-compose)
    ├── django (blue/green) - ports 8000/8001
    └── sites_datasette (blue/green) - ports 40001/40002
```

### Deploy Methods (Current)

| Service | Method | Blue-Green |
|---------|--------|------------|
| civic-observer | Manual SSH + script | Partial (script exists, not deployed) |
| corkboard django | CI + Python script | Yes (sequential restart) |
| corkboard datasette | Manual SSH | Yes (sequential restart) |
| Caddyfile | civic-band CI (clerk) | N/A |

### Problems

1. **Caddyfile conflict:** civic.observer is hardcoded to port 8888; blue-green deploy script tried to edit the Caddyfile, conflicting with civic-band CI
2. **No auto-failover:** Caddy doesn't health-check backends; if a service dies overnight, traffic still routes to it
3. **Manual rollbacks:** No easy "undo" button; requires manual intervention
4. **No visibility:** No notifications or monitoring for deploy success/failure
5. **Inconsistent patterns:** civic-observer deploys differently than corkboard

## Design

### 1. Caddyfile Changes

#### 1a. Add health endpoint to corkboard

Corkboard currently has no `/health/` endpoint. Add one:

```python
# corkboard/config/views.py
from django.http import JsonResponse


def health_check(request):
    return JsonResponse({"status": "ok"}, status=200)
```

```python
# corkboard/config/urls.py
from config.views import health_check

urlpatterns = [
    # ... existing paths ...
    path("health/", health_check, name="health_check"),
]
```

#### 1b. Update Caddyfile template (in civic-band/clerk)

Add health checking to all services. Create a reusable snippet:

```caddy
(health-proxy) {
    lb_policy cookie
    lb_retries 3
    health_uri /health/
    health_interval 10s
    health_timeout 5s
}

civic.observer {
    reverse_proxy 127.0.0.1:8888 127.0.0.1:8889 {
        import health-proxy
    }
}

(django-app) {
    import block_brazilians
    import subdomain-log
    reverse_proxy localhost:8000 localhost:8001 {
        import health-proxy
    }
}

civic.band {
    import subdomain-log civic.band
    import block_brazilians
    root * static
    route {
        file_server /how.html
        file_server /why.html
        file_server /privacy.html
        file_server /rss.xml
        reverse_proxy * 127.0.0.1:40001 127.0.0.1:40002 {
            import health-proxy
        }
    }
}
```

**Key benefits:**
- Both blue and green ports always listed for every service
- Caddy automatically routes to healthy backends
- If one backend dies, traffic auto-fails to the other
- Deploy scripts no longer need to edit Caddyfile

### 2. Simplified Deploy Script

#### 2a. New deploy flow (6 steps, no Caddyfile editing)

```
1. Pull new image
2. Run migrations
3. Start new color (blue or green)
4. Health check (wait for /health/ to return 200)
5. Stop old color (after optional grace period)
6. Update state file
```

The `update_caddy()` function is removed entirely.

#### 2b. Rollback strategy

With both colors always in Caddyfile, rollback is simple:

```bash
rollback() {
    OLD_COLOR=$(get_previous_color)
    OLD_PORT=$(get_port $OLD_COLOR)

    # Start old color back up
    docker-compose -f docker-compose.$OLD_COLOR.yml up -d

    # Wait for health
    health_check $OLD_PORT

    # Stop the bad deploy
    docker-compose -f docker-compose.$CURRENT_COLOR.yml down

    # Update state
    echo "$OLD_COLOR" > $STATE_FILE

    notify "rollback" "Rolled back to $OLD_COLOR"
}
```

No Caddyfile changes needed. Caddy routes to whichever backend is healthy.

#### 2c. Warm standby period (canary-style)

Keep old color running for 10 minutes after deploy. With both containers healthy, Caddy routes traffic to both — effectively a canary deployment:

- `lb_policy cookie` keeps existing users sticky to their backend
- New users get load balanced between old and new (~50/50)
- If new version has bugs, only ~50% of traffic is affected
- Rollback is instant — just stop the new container

```bash
# After successful health check of new color:
echo "Canary period: both $OLD_COLOR and $TARGET_COLOR serving traffic..."
echo "Keeping $OLD_COLOR running for 10 minutes..."

# Monitor for issues during canary period
sleep 600

# Check if we should rollback (manual intervention or automated check)
if [ -f "$DEPLOY_DIR/.rollback-requested" ]; then
    # Instant rollback - just stop the new deployment
    docker-compose -f docker-compose.$TARGET_COLOR.yml down
    notify "rollback" "Rolled back: stopped $TARGET_COLOR, $OLD_COLOR still serving"
else
    # Canary successful - shut down old version
    docker-compose -f docker-compose.$OLD_COLOR.yml down
    notify "success" "Canary complete: $TARGET_COLOR now sole backend"
fi
```

### 3. CI Automation + Unified Pattern

#### 3a. Unified deploy pattern

All services deploy the same way:

```
CI builds image → CI pushes to registry → CI SSHs to VPS → Deploy script runs
```

#### 3b. Shared deploy script (public-works repo)

Deploy scripts live in a separate `public-works` repository and are deployed to the VPS. Service repos (civic-observer, corkboard) just SSH in and call the scripts that are already on the server.

**public-works repo structure:**

```
public-works/
├── scripts/
│   ├── deploy.sh           # Main deploy script
│   ├── rollback.sh         # Rollback helper
│   └── lib/
│       ├── colors.sh       # Blue/green logic
│       ├── health.sh       # Health check functions
│       └── notify.sh       # Notification functions
└── .github/workflows/
    └── sync-to-server.yml  # Deploys scripts to VPS
```

**The main deploy script:**

```bash
#!/bin/bash
# /home/deploy/scripts/deploy.sh

SERVICE="$1"      # civic-observer, corkboard-django, corkboard-datasette
VERSION="$2"      # git sha or tag

case "$SERVICE" in
    civic-observer)
        DEPLOY_DIR="/home/deploy/civic-observer"
        COMPOSE_BASE="docker-compose.production-base.yml"
        PORTS=(8888 8889)
        ;;
    corkboard-django)
        DEPLOY_DIR="/home/deploy/corkboard"
        COMPOSE_BASE="docker-compose.yml"
        PORTS=(8000 8001)
        ;;
    corkboard-datasette)
        DEPLOY_DIR="/home/deploy/corkboard"
        COMPOSE_BASE="docker-compose.yml"
        PORTS=(40001 40002)
        ;;
esac

# ... rest of deploy logic using these variables
```

**public-works CI syncs scripts to server:**

```yaml
# public-works/.github/workflows/sync-to-server.yml
name: Sync scripts to server

on:
  push:
    branches: [main]

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Sync scripts to VPS
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.VPS_HOST }}
          username: deploy
          key: ${{ secrets.DEPLOY_SSH_KEY }}
          source: "scripts/*"
          target: "/home/deploy/"
          strip_components: 0
```

**Benefits:**
- Service CIs stay simple — just SSH and call `/home/deploy/scripts/deploy.sh`
- Update deploy logic once in public-works, all services pick it up automatically
- No cross-repo access tokens needed
- Scripts are version-controlled and deployed like any other code

#### 3c. GitHub Actions for civic-observer

```yaml
# .github/workflows/deploy-production.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Build and push Docker image
        run: |
          docker build -t civic-observer:${{ github.sha }} .
          # Push to registry...

  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Deploy via SSH
        uses: appleboy/ssh-action@v1
        with:
          host: ${{ secrets.VPS_HOST }}
          username: deploy
          key: ${{ secrets.DEPLOY_SSH_KEY }}
          script: |
            /home/deploy/scripts/deploy.sh civic-observer ${{ github.sha }}
```

#### 3d. Deploy locking

Prevent simultaneous deploys with a lockfile:

```bash
# At start of deploy script
LOCKFILE="/tmp/civic-deploy.lock"
exec 200>"$LOCKFILE"
flock -n 200 || { echo "Another deploy in progress"; exit 1; }
```

### 4. Visibility + Monitoring

#### 4a. Deploy notifications

Add Slack/Discord webhook notifications:

```bash
notify() {
    local status="$1"
    local message="$2"

    curl -X POST "$SLACK_WEBHOOK_URL" \
        -H 'Content-type: application/json' \
        -d "{\"text\": \"[$status] $SERVICE: $message\"}"
}

# Usage:
notify "deploying" "Starting deploy of $VERSION"
notify "success" "Deploy complete: $VERSION"
notify "failure" "Deploy failed: $ERROR_MESSAGE"
notify "rollback" "Rolled back to $OLD_VERSION"
```

#### 4b. External health monitoring

Set up UptimeRobot (free tier) to monitor:
- `https://civic.observer/health/`
- `https://civic.band/health/`

Alerts via email/SMS when endpoints go down.

#### 4c. Caddy metrics (optional, for deeper visibility)

Expose Prometheus metrics endpoint:

```caddy
:9180 {
    metrics /metrics
}
```

Can later add Prometheus + Grafana for dashboards showing request rates, error rates, and latencies.

#### 4d. Deploy history log

Append deploy events to a log file:

```bash
log_deploy() {
    echo "$(date -Iseconds) | $SERVICE | $VERSION | $STATUS | $COLOR" \
        >> /home/deploy/deploy-history.log
}
```

Provides simple audit trail:
```
2024-11-27T14:32:01 | civic-observer | abc123 | success | green
2024-11-27T14:35:22 | corkboard-django | def456 | success | blue
2024-11-27T15:01:03 | civic-observer | bad789 | failure | blue
2024-11-27T15:02:15 | civic-observer | abc123 | rollback | green
```

## Implementation Order

1. **Add health endpoint to corkboard** (unblocks everything else)
2. **Update Caddyfile template in civic-band** (add health-proxy snippet, dual ports)
3. **Create public-works repo** with shared deploy script
4. **Set up public-works CI** to sync scripts to VPS
5. **Simplify civic-observer deploy script** (remove Caddyfile editing, use shared script)
6. **Set up UptimeRobot** (quick win for visibility)
7. **Add deploy notifications** (Slack/Discord webhook in shared script)
8. **Add CI automation to civic-observer** (GitHub Actions calling shared script)
9. **Migrate corkboard to shared deploy script**
10. **Add deploy locking** (prevent collisions)
11. **Add warm standby period** (10 min canary with instant rollback)
12. **Optional: Caddy metrics + Grafana** (deeper observability)

## Decisions Made

- **Shared deploy script location:** Separate `public-works` repo, synced to VPS via CI. Service repos SSH in and call scripts already on the server.
- **Notification channel:** Slack webhook
- **Warm standby period:** 10 minutes, canary-style (both old and new serve traffic). Rollback is instant — just stop the new container.
